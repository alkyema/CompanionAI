{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7879155,"sourceType":"datasetVersion","datasetId":4624298},{"sourceId":9220402,"sourceType":"datasetVersion","datasetId":5575883},{"sourceId":1531926,"sourceType":"datasetVersion","datasetId":903283},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-09-07T09:59:52.576231Z","iopub.execute_input":"2024-09-07T09:59:52.577358Z","iopub.status.idle":"2024-09-07T10:02:05.457956Z","shell.execute_reply.started":"2024-09-07T09:59:52.577289Z","shell.execute_reply":"2024-09-07T10:02:05.456667Z"},"editable":false,"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\n","metadata":{"id":"VLzgZ14X_rMs","execution":{"iopub.status.busy":"2024-09-07T10:02:57.653008Z","iopub.execute_input":"2024-09-07T10:02:57.654176Z","iopub.status.idle":"2024-09-07T10:02:57.659640Z","shell.execute_reply.started":"2024-09-07T10:02:57.654123Z","shell.execute_reply":"2024-09-07T10:02:57.658579Z"},"editable":false,"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:03:43.856495Z","iopub.execute_input":"2024-09-07T10:03:43.857177Z","iopub.status.idle":"2024-09-07T10:03:44.146778Z","shell.execute_reply.started":"2024-09-07T10:03:43.857138Z","shell.execute_reply":"2024-09-07T10:03:44.145770Z"},"editable":false,"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on FAQ Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"id":"na9CAoHC5gM9","execution":{"iopub.status.busy":"2024-09-07T10:03:53.181785Z","iopub.execute_input":"2024-09-07T10:03:53.182488Z","iopub.status.idle":"2024-09-07T10:04:12.559735Z","shell.execute_reply.started":"2024-09-07T10:03:53.182442Z","shell.execute_reply":"2024-09-07T10:04:12.558607Z"},"editable":false,"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msatwikkishore6953\u001b[0m (\u001b[33msatwikkishore\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240907_100355-rcfsf1y0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/satwikkishore/Fine-tune%20Llama%203%208B%20on%20FAQ%20Dataset/runs/rcfsf1y0' target=\"_blank\">graceful-aardvark-8</a></strong> to <a href='https://wandb.ai/satwikkishore/Fine-tune%20Llama%203%208B%20on%20FAQ%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/satwikkishore/Fine-tune%20Llama%203%208B%20on%20FAQ%20Dataset' target=\"_blank\">https://wandb.ai/satwikkishore/Fine-tune%20Llama%203%208B%20on%20FAQ%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/satwikkishore/Fine-tune%20Llama%203%208B%20on%20FAQ%20Dataset/runs/rcfsf1y0' target=\"_blank\">https://wandb.ai/satwikkishore/Fine-tune%20Llama%203%208B%20on%20FAQ%20Dataset/runs/rcfsf1y0</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ndataset_name = \"/kaggle/input/mental-health-counseling-conversations-k/archive/Dataset.csv\"\nnew_model = \"llama-3-8b-chat-MH\"","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:16:32.301117Z","iopub.execute_input":"2024-09-07T10:16:32.301569Z","iopub.status.idle":"2024-09-07T10:16:32.308022Z","shell.execute_reply.started":"2024-09-07T10:16:32.301528Z","shell.execute_reply":"2024-09-07T10:16:32.306790Z"},"editable":false,"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:16:36.474842Z","iopub.execute_input":"2024-09-07T10:16:36.475267Z","iopub.status.idle":"2024-09-07T10:16:36.483601Z","shell.execute_reply.started":"2024-09-07T10:16:36.475223Z","shell.execute_reply":"2024-09-07T10:16:36.482342Z"},"editable":false,"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"id":"StJKGiDDHzdk","outputId":"871214ba-6c30-4ecf-ac68-550f296b7ef6","execution":{"iopub.status.busy":"2024-09-07T10:16:39.805083Z","iopub.execute_input":"2024-09-07T10:16:39.806128Z","iopub.status.idle":"2024-09-07T10:18:00.261944Z","shell.execute_reply.started":"2024-09-07T10:16:39.806068Z","shell.execute_reply":"2024-09-07T10:18:00.260710Z"},"editable":false,"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be51bd36a8b4d418fd5cf88aa59c57a"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\ndf=messages = pd.read_csv(dataset_name)\n# df = df[['instruction','category','intent','response']]\n# for i,j in df.iterrows():\n#     j['instruction'],j['response'] = j['instruction'].replace(\"{{Order Number}}\", \"\"),j['response'].replace(\"{{Order Number}}\", \"\").replace(\"{{\",\"\").replace(\"}}\",\"\")\n# df = df[df['category'] != 'SHIPPING']\n# new_df  = pd.DataFrame()\n# count = 0\n# for j in df['category'].unique():\n#     a = df[df['category'] == j]\n#     if count == 2:\n#         break\n#     for i in a['intent'].unique():\n#         temp_df = df.loc[df['intent'] == i].head(100)\n#         new_df = pd.concat([new_df, temp_df],ignore_index = True)\n#     count += 1\n\n# df = new_df\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:18:00.264431Z","iopub.execute_input":"2024-09-07T10:18:00.265163Z","iopub.status.idle":"2024-09-07T10:18:00.385891Z","shell.execute_reply.started":"2024-09-07T10:18:00.265100Z","shell.execute_reply":"2024-09-07T10:18:00.384760Z"},"editable":false,"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                Context  \\\n3507  My grandson's step-mother sends him to school ...   \n3508  My boyfriend is in recovery from drug addictio...   \n3509  The birth mother attempted suicide several tim...   \n3510  I think adult life is making him depressed and...   \n3511  I just took a job that requires me to travel f...   \n\n                                               Response  \n3507  Absolutely not! It is never in a child's best ...  \n3508  I'm sorry you have tension between you and you...  \n3509  The true answer is, \"no one can really say wit...  \n3510  How do you help yourself to believe you requir...  \n3511                           hmm this is a tough one!  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3507</th>\n      <td>My grandson's step-mother sends him to school ...</td>\n      <td>Absolutely not! It is never in a child's best ...</td>\n    </tr>\n    <tr>\n      <th>3508</th>\n      <td>My boyfriend is in recovery from drug addictio...</td>\n      <td>I'm sorry you have tension between you and you...</td>\n    </tr>\n    <tr>\n      <th>3509</th>\n      <td>The birth mother attempted suicide several tim...</td>\n      <td>The true answer is, \"no one can really say wit...</td>\n    </tr>\n    <tr>\n      <th>3510</th>\n      <td>I think adult life is making him depressed and...</td>\n      <td>How do you help yourself to believe you requir...</td>\n    </tr>\n    <tr>\n      <th>3511</th>\n      <td>I just took a job that requires me to travel f...</td>\n      <td>hmm this is a tough one!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\ndataset = Dataset.from_pandas(df)\n\nfor i in range(3):\n    print(dataset[i])","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:27:55.686265Z","iopub.execute_input":"2024-09-07T10:27:55.686727Z","iopub.status.idle":"2024-09-07T10:27:55.741350Z","shell.execute_reply.started":"2024-09-07T10:27:55.686686Z","shell.execute_reply":"2024-09-07T10:27:55.739904Z"},"editable":false,"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"{'Context': \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\", 'Response': \"If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media. \\xa0Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living. \\xa0They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible. \\xa0 Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\", '__index_level_0__': 0}\n{'Context': \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\", 'Response': 'Hello, and thank you for your question and seeking advice on this. Feelings of worthlessness is unfortunately common. In fact, most people, if not all, have felt this to some degree at some point in their life. You are not alone.\\xa0Changing our feelings is like changing our thoughts - it\\'s hard to do. Our minds are so amazing that the minute you change your thought another one can be right there to take it\\'s place. Without your permission, another thought can just pop in there. The new thought may feel worse than the last one! My guess is that you have tried several things to improve this on your own even before reaching out on here. People often try thinking positive thoughts, debating with their thoughts, or simply telling themselves that they need to \"snap out of it\" - which is also a thought that carries some self-criticism.\\xa0Some people try a different approach, and there are counselors out there that can help you with this. The idea is that instead of trying to change the thoughts, you change how you respond to them. You learn skills that allow you to manage difficult thoughts and feelings differently so they don\\'t have the same impact on you that they do right now. For some people, they actually DO begin to experience less hurtful thoughts once they learn how to manage the ones they have differently. Acceptance and Commitment Therapy may be a good choice for you.\\xa0There is information online and even self-help books that you can use to teach you the skills that I mentioned. Because they are skills, they require practice, but many people have found great relief and an enriched life by learning them.\\xa0As for suicidal thoughts, I am very glad to read that this has not happened to you. Still, you should watch out for this because it can be a sign of a worsening depression. If you begin to think about this, it is important to reach out to a support system right away. The National Suicide Prevention Lifeline is 1-800-273-8255. The text line is #741741.\\xa0I hope some other colleagues will provide you more suggestions.\\xa0Be well...Robin Landwehr, DBH, LPCC', '__index_level_0__': 1}\n{'Context': \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\", 'Response': \"First thing I'd suggest is getting the sleep you need or it will impact how you think and feel. I'd look at finding what is going well in your life and what you can be grateful for. I believe everyone has talents and wants to find their purpose in life. I think you can figure it out with some help.\", '__index_level_0__': 2}\n","output_type":"stream"}]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:27:58.621151Z","iopub.execute_input":"2024-09-07T10:27:58.621864Z","iopub.status.idle":"2024-09-07T10:27:59.452242Z","shell.execute_reply.started":"2024-09-07T10:27:58.621818Z","shell.execute_reply":"2024-09-07T10:27:59.450915Z"},"editable":false,"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:27:33.449042Z","iopub.execute_input":"2024-09-07T10:27:33.449909Z","iopub.status.idle":"2024-09-07T10:27:33.461903Z","shell.execute_reply.started":"2024-09-07T10:27:33.449850Z","shell.execute_reply":"2024-09-07T10:27:33.460790Z"},"editable":false,"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:27:34.923913Z","iopub.execute_input":"2024-09-07T10:27:34.924349Z","iopub.status.idle":"2024-09-07T10:27:34.940847Z","shell.execute_reply.started":"2024-09-07T10:27:34.924288Z","shell.execute_reply":"2024-09-07T10:27:34.939488Z"},"editable":false,"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Context     0\nResponse    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":" # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Context\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Response\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= 4,\n)\n\ndataset","metadata":{"id":"XzF2UjPvTBag","outputId":"3733e45f-605e-4564-88c7-368c9c5bf9cd","execution":{"iopub.status.busy":"2024-09-07T10:28:13.600825Z","iopub.execute_input":"2024-09-07T10:28:13.601290Z","iopub.status.idle":"2024-09-07T10:28:14.811197Z","shell.execute_reply.started":"2024-09-07T10:28:13.601250Z","shell.execute_reply":"2024-09-07T10:28:14.809715Z"},"editable":false,"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/3508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d87bdaf1fa0844d083d40e4b800b6462"}},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Context', 'Response', '__index_level_0__', 'text'],\n    num_rows: 3508\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:28:18.166449Z","iopub.execute_input":"2024-09-07T10:28:18.166931Z","iopub.status.idle":"2024-09-07T10:28:18.194150Z","shell.execute_reply.started":"2024-09-07T10:28:18.166884Z","shell.execute_reply":"2024-09-07T10:28:18.193039Z"},"editable":false,"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"\"<|im_start|>user\\nI'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?<|im_end|>\\n<|im_start|>assistant\\nTherapy is essential for those that are feeling depressed and worthless. When I work with those that are experiencing concerns related to feeling of depression and issues with self esteem. I generally work with my client to help build coping skills to reduce level of depression and to assist with strengthening \\xa0self esteem, by guiding my client with CBT practices. CBT helps with gaining a better awareness of how your thought process influences your\\xa0belief system, and how your beliefs impact your actions and the outcome of your behaviors. \\xa0This process isn’t easy but it helps teach an individual that we don’t always have control over what happens in our lives but we can control how we interpret, feel, and behave. CBT is good for individuals dealing with depression, anxiety, toxic relationships, stress, self esteem, codependency, etc.<|im_end|>\\n\""},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:28:24.938599Z","iopub.execute_input":"2024-09-07T10:28:24.939053Z","iopub.status.idle":"2024-09-07T10:28:24.961273Z","shell.execute_reply.started":"2024-09-07T10:28:24.939009Z","shell.execute_reply":"2024-09-07T10:28:24.960383Z"},"editable":false,"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=6,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=11,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=True,  # Enable mixed precision\n    bf16=False, # Use bf16 if supported by your hardware (alternative to fp16)\n    group_by_length=True,\n    report_to=\"wandb\"\n)\n","metadata":{"id":"peOnLAAhS0y1","execution":{"iopub.status.busy":"2024-09-07T10:33:42.455879Z","iopub.execute_input":"2024-09-07T10:33:42.456343Z","iopub.status.idle":"2024-09-07T10:33:42.492287Z","shell.execute_reply.started":"2024-09-07T10:33:42.456282Z","shell.execute_reply":"2024-09-07T10:33:42.491209Z"},"editable":false,"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\nmodel.gradient_checkpointing_enable()  # Enable gradient checkpointing\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:34:24.462285Z","iopub.execute_input":"2024-09-07T10:34:24.463357Z","iopub.status.idle":"2024-09-07T10:34:28.436636Z","shell.execute_reply.started":"2024-09-07T10:34:24.463287Z","shell.execute_reply":"2024-09-07T10:34:28.435491Z"},"editable":false,"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3157 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97f953bdbfc543158ae7c266e3649cdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/351 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21546cb5f635498c88c6485b765bc146"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:37:53.853073Z","iopub.execute_input":"2024-09-07T10:37:53.853854Z","iopub.status.idle":"2024-09-07T10:37:55.013000Z","shell.execute_reply.started":"2024-09-07T10:37:53.853809Z","shell.execute_reply":"2024-09-07T10:37:55.011694Z"},"editable":false,"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:38:38.602273Z","iopub.execute_input":"2024-09-07T10:38:38.602721Z","iopub.status.idle":"2024-09-07T10:38:38.608757Z","shell.execute_reply.started":"2024-09-07T10:38:38.602681Z","shell.execute_reply":"2024-09-07T10:38:38.607384Z"},"editable":false,"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"for param in model.parameters():\n    if param.dtype in [torch.float32, torch.float64, torch.complex64, torch.complex128]:\n        param.requires_grad = True\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:41:51.645603Z","iopub.execute_input":"2024-09-07T10:41:51.646358Z","iopub.status.idle":"2024-09-07T10:41:51.667370Z","shell.execute_reply.started":"2024-09-07T10:41:51.646291Z","shell.execute_reply":"2024-09-07T10:41:51.666459Z"},"editable":false,"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    if not param.requires_grad:\n        print(f\"Parameter {name} does not require gradients. Dtype: {param.dtype}\")\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-07T10:43:16.721855Z","iopub.execute_input":"2024-09-07T10:43:16.722273Z","iopub.status.idle":"2024-09-07T10:43:16.783699Z","shell.execute_reply.started":"2024-09-07T10:43:16.722234Z","shell.execute_reply":"2024-09-07T10:43:16.782358Z"},"editable":false,"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Parameter base_model.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.0.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.0.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.0.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.0.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.1.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.1.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.1.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.1.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.2.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.2.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.2.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.2.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.3.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.3.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.3.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.3.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.4.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.4.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.4.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.4.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.5.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.5.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.5.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.5.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.6.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.6.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.6.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.6.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.7.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.7.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.7.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.7.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.8.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.8.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.8.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.8.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.9.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.9.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.9.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.9.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.10.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.10.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.10.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.10.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.11.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.11.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.11.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.11.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.12.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.12.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.12.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.12.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.13.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.13.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.13.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.13.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.14.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.14.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.14.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.14.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.15.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.15.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.15.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.15.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.16.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.16.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.16.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.16.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.17.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.17.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.17.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.17.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.18.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.18.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.18.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.18.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.19.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.19.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.19.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.19.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.20.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.20.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.20.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.20.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.21.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.21.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.21.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.21.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.22.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.22.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.22.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.22.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.23.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.23.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.23.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.23.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.24.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.24.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.24.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.24.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.25.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.25.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.25.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.25.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.26.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.26.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.26.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.26.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.27.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.27.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.27.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.27.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.28.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.28.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.28.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.28.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.29.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.29.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.29.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.29.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.30.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.30.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.30.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.30.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.31.mlp.up_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.31.mlp.down_proj.base_layer.weight does not require gradients. Dtype: torch.uint8\nParameter base_model.model.base_model.model.model.layers.31.input_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.layers.31.post_attention_layernorm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.model.norm.weight does not require gradients. Dtype: torch.float16\nParameter base_model.model.base_model.model.lm_head.weight does not require gradients. Dtype: torch.float16\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-07T10:38:40.378974Z","iopub.execute_input":"2024-09-07T10:38:40.379418Z","iopub.status.idle":"2024-09-07T10:38:50.126893Z","shell.execute_reply.started":"2024-09-07T10:38:40.379373Z","shell.execute_reply":"2024-09-07T10:38:50.123401Z"},"editable":false,"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:450\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 450\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2192\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"],"ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","output_type":"error"}]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"id":"nKgZBEGVS5a2","execution":{"iopub.status.busy":"2024-06-20T11:37:40.774974Z","iopub.execute_input":"2024-06-20T11:37:40.77536Z","iopub.status.idle":"2024-06-20T11:37:45.561092Z","shell.execute_reply.started":"2024-06-20T11:37:40.77533Z","shell.execute_reply":"2024-06-20T11:37:45.560306Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:37:57.73096Z","iopub.execute_input":"2024-06-20T11:37:57.73136Z","iopub.status.idle":"2024-06-20T11:38:13.974915Z","shell.execute_reply.started":"2024-06-20T11:37:57.731328Z","shell.execute_reply":"2024-06-20T11:38:13.973811Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\": \"I want to cancle my order\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"id":"L7vHP41ITQPb","execution":{"iopub.status.busy":"2024-06-20T11:38:56.151832Z","iopub.execute_input":"2024-06-20T11:38:56.152244Z","iopub.status.idle":"2024-06-20T11:39:15.563563Z","shell.execute_reply.started":"2024-06-20T11:38:56.152211Z","shell.execute_reply":"2024-06-20T11:39:15.562547Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\": \"I want to replace my order i got wrong delavery\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:39:43.68019Z","iopub.execute_input":"2024-06-20T11:39:43.680585Z","iopub.status.idle":"2024-06-20T11:40:01.962073Z","shell.execute_reply.started":"2024-06-20T11:39:43.680555Z","shell.execute_reply":"2024-06-20T11:40:01.960986Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]}]}