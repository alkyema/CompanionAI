{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.licdn.com/dms/image/D4D12AQGkHY89zhGDZA/article-cover_image-shrink_720_1280/0/1691431976579?e=2147483647&v=beta&t=lgk0jF-6kju4RyG68tF_4cxZklJqF3rtXrbQEcEYX7c\">\n",
    "\n",
    "\n",
    "# <b><span style='color:#F1A424'>|</span> Mission: <span style='color:#F1A424'>Llama2 Fine-Tuning</span><span style='color:#ABABAB'>\n",
    "\n",
    "***\n",
    "\n",
    "**Please consider upvoting the notebook if you found it useful** \n",
    "\n",
    "Welcome, Kaggle enthusiasts and AI pioneers! In the last year, we have witnessed an unprecedented boom in technology, particularly in the realm of Large Language Models (LLMs) and broader Artificial Intelligence (AI) systems. Tools like ChatGPT have not just entered the mainstream but have revolutionized it, reshaping how we gather and process information across various sectors.\n",
    "\n",
    "In this notebook, we dive into the heart of this technological revolution by focusing on a specific aspect of AI - Large Language Models, with a special emphasis on a model we're referring to as \"Llama2\". You will embark on a journey to understand and master the art of fine-tuning Llama2 for text generation. By leveraging PyTorch, we aim to equip you with the skills and knowledge to harness the full potential of LLMs.\n",
    "    \n",
    "Hope you enjoy it and find it useful.\n",
    "    \n",
    "\n",
    "### <b><span style='color:#F1A424'>Table of Contents</span></b> <a class='anchor' id='top'></a>\n",
    "<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n",
    "<li> <a href=\"#install_libraries\">Install libraries</a></li>\n",
    "<li><a href=\"#import_libraries\">Import Libraries</a></li>\n",
    "<li><a href=\"#load_data\">Load Data</a></li>\n",
    "<li><a href=\"#configuration\">Configuration</a></li>\n",
    "<li><a href=\"#configure_parameters\">Configure Quantization and LORA-specific parameters</a></li>\n",
    "<li><a href=\"#load_model\">Load Model</a></li>\n",
    "<li><a href=\"#training\">Training</a></li>\n",
    "<li><a href=\"#testing\">Testing</a></li>\n",
    "<li><a href=\"#save_model\">Saving model for inference</a></li>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Install Libraries</b><a class='anchor' id='install_libraries'></a> [↑](#top) \n",
    "\n",
    "***\n",
    "\n",
    "Install all the required libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (0.40.2)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.43.3-py3-none-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from bitsandbytes) (2.4.1+cu121)\n",
      "Requirement already satisfied: numpy in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from bitsandbytes) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from torch->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from torch->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Using cached bitsandbytes-0.43.3-py3-none-win_amd64.whl (136.5 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.40.2\n",
      "    Uninstalling bitsandbytes-0.40.2:\n",
      "      Successfully uninstalled bitsandbytes-0.40.2\n",
      "Successfully installed bitsandbytes-0.43.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++ BUG REPORT INFORMATION ++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++\n",
      "CUDA specs: CUDASpecs(highest_compute_capability=(8, 6), cuda_version_string='121', cuda_version_tuple=(12, 1))\n",
      "PyTorch settings found: CUDA_VERSION=121, Highest Compute Capability: (8, 6).\n",
      "To manually override the PyTorch CUDA version please see: https://github.com/TimDettmers/bitsandbytes/blob/main/docs/source/nonpytorchcuda.mdx\n",
      "CUDA SETUP: WARNING! CUDA runtime files not found in any environmental path.\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Checking that the library is importable and CUDA is callable...\n",
      "SUCCESS!\n",
      "Installation was successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The directory listed in your path is found to be non-existent: \\ROBOTICS_1\n"
     ]
    }
   ],
   "source": [
    "!python -m bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Import Libraries</b><a class='anchor' id='import_libraries'></a> [↑](#top) \n",
    "\n",
    "***\n",
    "\n",
    "Import all the required libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GCWf-ANoqlgr"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Configuration</b><a class='anchor' id='configuration'></a> [↑](#top) \n",
    "\n",
    "***\n",
    "\n",
    "Central repository for this notebook's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bqfbhUZI-4c_"
   },
   "outputs": [],
   "source": [
    "# Set up model configuration and training parameters\n",
    "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
    "dataset_name = r\"C:\\Users\\robot\\Downloads\\llama\\archive\\train1.jsonl\"\n",
    "new_model = \"llama-2-7b-MentalHealthCare\"\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "output_dir = \"./results\"\n",
    "num_train_epochs = 2\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 25\n",
    "logging_steps = 5\n",
    "max_seq_length = None\n",
    "packing = False\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = load_dataset('json', data_files=r\"C:\\Users\\robot\\Downloads\\llama\\archive\\test1.jsonl\", split=\"train\")\n",
    "valid_dataset = load_dataset('json', data_files= r\"C:\\Users\\robot\\Downloads\\llama\\archive\\train1.jsonl\", split=\"train\")\n",
    "\n",
    "# Preprocess datasets\n",
    "train_dataset_mapped = train_dataset.map(lambda examples: {'text': [input_text + ' [/INST] ' + output_text for input_text, output_text in zip(examples['input'], examples['output'])]},batched=True)\n",
    "valid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [input_text + ' [/INST] ' + output_text for input_text, output_text in zip(examples['input'], examples['output'])]},batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Configuration of Quantization and LORA parameters</b><a class='anchor' id='configure_parameters'></a> [↑](#top) \n",
    "\n",
    "***\n",
    "\n",
    "As model size is big it is loaded in 4 bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qf1qxbiF-x6p",
    "outputId": "1bc5b55f-9866-480d-c8d6-582996a68910"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34022b440d3b476eb9660496e310d103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure quantization parameters\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Load pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Configure LoRA-specific parameters\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Training</b><a class='anchor' id='training'></a> [↑](#top) \n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages\\peft\\utils\\other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\robot\\anaconda3\\envs\\my_cuda_env\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/468 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"all\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50  # Evaluate every 50 steps\n",
    ")\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset_mapped,\n",
    "    eval_dataset=valid_dataset_mapped, \n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Testing</b><a class='anchor' id='testing'></a> [↑](#top) \n",
    "\n",
    "***\n",
    "\n",
    "Testing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KJI9TVUDYZr",
    "outputId": "ce4564d6-7868-4307-90b5-64828c99407a"
   },
   "outputs": [],
   "source": [
    "# Suppress logging messages to avoid unnecessary output\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Create text generation pipelines using the specified model and tokenizer\n",
    "# Define two pipelines with different maximum lengths\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=250)\n",
    "pipe2 = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "\n",
    "# Initialize an empty list to store generated text\n",
    "generated_text = []\n",
    "\n",
    "# Iterate over the test data\n",
    "for i in tqdm(range(len(final_test_data))):\n",
    "    # Extract the prompt from the test data\n",
    "    prompt = final_test_data['prompt'].iloc[i]\n",
    "    \n",
    "    # Attempt to generate text using the first pipeline with a max length of 250\n",
    "    try:\n",
    "        result = pipe(prompt)\n",
    "        # Append the generated text to the list, extracting the relevant part after '[/INST]'\n",
    "        generated_text.append(result[0]['generated_text'].split('[/INST]')[1])\n",
    "    except:\n",
    "        # If an exception occurs, try the second pipeline with a max length of 500\n",
    "        try:\n",
    "            result = pipe2(prompt)\n",
    "            # Append the generated text to the list, extracting the relevant part after '[/INST]'\n",
    "            generated_text.append(result[0]['generated_text'].split('[/INST]')[1])\n",
    "        except:\n",
    "            # If both pipelines fail, append a default placeholder text\n",
    "            generated_text.append(\"ABCD1234@#\")\n",
    "\n",
    "# The 'generated_text' list now contains the generated text for each prompt in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhqQFeAHhb_Y"
   },
   "outputs": [],
   "source": [
    "# Assign the generated text to a new column 'generated_text' in the 'final_test_data' DataFrame\n",
    "final_test_data['generated_text'] = generated_text\n",
    "\n",
    "# Reset the index of the DataFrame for a cleaner representation in the CSV file\n",
    "final_test_data = final_test_data.reset_index(drop=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file at the specified path\n",
    "final_test_data.to_csv('/content/drive/MyDrive/llama2_finetune_output_1128.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style='color:#F1A424'>|</span> Saving Model for inference</b><a class='anchor' id='save_model'></a> [↑](#top) \n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JutZuGdRq1D_",
    "outputId": "599232c5-d743-4513-b2cc-fe5dc399b2be"
   },
   "outputs": [],
   "source": [
    "# Set the path where the merged model will be saved\n",
    "model_path = \"/content/drive/MyDrive/llama-2-7b-custom\" \n",
    "\n",
    "# Reload the base model in FP16 and configure settings\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,  \n",
    "    return_dict=True,        \n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map=device_map,    \n",
    ")\n",
    "\n",
    "# Instantiate a PeftModel using the base model and the new model\n",
    "model = PeftModel.from_pretrained(base_model, new_model)  # Combine the base model and the fine-tuned weights\n",
    "\n",
    "# Merge the base model with LoRA weights and unload unnecessary parts\n",
    "model = model.merge_and_unload()  # Finalize the model by merging and unloading any redundant components\n",
    "\n",
    "# Reload the tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) \n",
    "tokenizer.pad_token = tokenizer"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5666674,
     "sourceId": 9348857,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5669570,
     "sourceId": 9352801,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "096ba04a1d7f45ec84f06a7a9f8276b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "251ff2c9141c4da99f51008dfbee9b04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_096ba04a1d7f45ec84f06a7a9f8276b7",
      "placeholder": "​",
      "style": "IPY_MODEL_d4a6149f85a14809b2ec8162722a654c",
      "value": " 2/2 [00:12&lt;00:00,  5.40s/it]"
     }
    },
    "528923440b3a482c9dc8c04157c687fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7086cff6a30948e5b4af029e653f8e71",
      "placeholder": "​",
      "style": "IPY_MODEL_644476f799be4a3f9354e3e5c47b6feb",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "5cfc4ffce7934bc983e9d1bd43b388b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_528923440b3a482c9dc8c04157c687fa",
       "IPY_MODEL_cb6170e69e4349c792f640fab25f8cf6",
       "IPY_MODEL_251ff2c9141c4da99f51008dfbee9b04"
      ],
      "layout": "IPY_MODEL_ece5939e218d4acca4aacf3e0ca6f947"
     }
    },
    "644476f799be4a3f9354e3e5c47b6feb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7086cff6a30948e5b4af029e653f8e71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b21e3f35d647483b8c11937e9f715702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cb6170e69e4349c792f640fab25f8cf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d008a6ef62db4bfb9e241c35ace92811",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b21e3f35d647483b8c11937e9f715702",
      "value": 2
     }
    },
    "d008a6ef62db4bfb9e241c35ace92811": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4a6149f85a14809b2ec8162722a654c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ece5939e218d4acca4aacf3e0ca6f947": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
