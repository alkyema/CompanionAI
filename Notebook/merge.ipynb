{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-21T11:29:25.988028Z","iopub.status.busy":"2024-06-21T11:29:25.987342Z","iopub.status.idle":"2024-06-21T11:30:36.225034Z","shell.execute_reply":"2024-06-21T11:30:36.223884Z","shell.execute_reply.started":"2024-06-21T11:29:25.987994Z"},"trusted":true},"outputs":[],"source":["%%capture\n","%pip install -U bitsandbytes\n","%pip install -U transformers\n","%pip install -U accelerate\n","%pip install -U peft\n","%pip install -U trl"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Collecting trl\n","  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torch>=1.4.0 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from trl) (2.3.1+cu121)\n","Requirement already satisfied: transformers>=4.31.0 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from trl) (4.41.2)\n","Requirement already satisfied: numpy>=1.18.2 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from trl) (1.26.3)\n","Requirement already satisfied: accelerate in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from trl) (0.21.0)\n","Collecting datasets (from trl)\n","  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n","Collecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: filelock in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from torch>=1.4.0->trl) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from torch>=1.4.0->trl) (4.12.2)\n","Requirement already satisfied: sympy in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from torch>=1.4.0->trl) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from torch>=1.4.0->trl) (3.2.1)\n","Requirement already satisfied: jinja2 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.3)\n","Requirement already satisfied: fsspec in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from torch>=1.4.0->trl) (2024.2.0)\n","Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from torch>=1.4.0->trl) (2021.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from transformers>=4.31.0->trl) (0.23.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from transformers>=4.31.0->trl) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from transformers>=4.31.0->trl) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from transformers>=4.31.0->trl) (2024.5.15)\n","Requirement already satisfied: requests in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from transformers>=4.31.0->trl) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from transformers>=4.31.0->trl) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from transformers>=4.31.0->trl) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from transformers>=4.31.0->trl) (4.66.4)\n","Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n","  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n","Collecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n","  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: colorama>=0.4.0 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from tyro>=0.5.11->trl) (0.4.6)\n","Collecting eval-type-backport>=0.1.3 (from tyro>=0.5.11->trl)\n","  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n","Requirement already satisfied: psutil in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from accelerate->trl) (5.9.8)\n","Collecting pyarrow>=15.0.0 (from datasets->trl)\n","  Downloading pyarrow-16.1.0-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n","Collecting pyarrow-hotfix (from datasets->trl)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->trl)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from datasets->trl) (2.2.2)\n","Collecting xxhash (from datasets->trl)\n","  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets->trl)\n","  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n","Collecting aiohttp (from datasets->trl)\n","  Downloading aiohttp-3.9.5-cp39-cp39-win_amd64.whl.metadata (7.7 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp->datasets->trl)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n","Collecting attrs>=17.3.0 (from aiohttp->datasets->trl)\n","  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n","Collecting frozenlist>=1.1.1 (from aiohttp->datasets->trl)\n","  Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->trl)\n","  Downloading multidict-6.0.5-cp39-cp39-win_amd64.whl.metadata (4.3 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->trl)\n","  Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl.metadata (32 kB)\n","Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->trl)\n","  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: intel-openmp==2021.* in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.4.0->trl) (2021.4.0)\n","Requirement already satisfied: tbb==2021.* in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.4.0->trl) (2021.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2024.6.2)\n","Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n","  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from pandas->datasets->trl) (2.9.0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from pandas->datasets->trl) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from pandas->datasets->trl) (2024.1)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n","Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n","  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: six>=1.5 in c:\\users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n","Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n","   ---------------------------------------- 0.0/226.7 kB ? eta -:--:--\n","   ---------------------------------------- 226.7/226.7 kB 4.7 MB/s eta 0:00:00\n","Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n","   ---------------------------------------- 0.0/102.4 kB ? eta -:--:--\n","   ---------------------------------------- 102.4/102.4 kB 5.8 MB/s eta 0:00:00\n","Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","   ---------------------------------------- 0.0/547.8 kB ? eta -:--:--\n","   --------------------------------------- 547.8/547.8 kB 16.8 MB/s eta 0:00:00\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n","   ---------------------------------------- 116.3/116.3 kB ? eta 0:00:00\n","Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n","Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n","Downloading aiohttp-3.9.5-cp39-cp39-win_amd64.whl (371 kB)\n","   ---------------------------------------- 0.0/371.6 kB ? eta -:--:--\n","   ---------------------------------------- 371.6/371.6 kB ? eta 0:00:00\n","Downloading pyarrow-16.1.0-cp39-cp39-win_amd64.whl (25.9 MB)\n","   ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n","   -- ------------------------------------- 1.8/25.9 MB 57.8 MB/s eta 0:00:01\n","   --- ------------------------------------ 2.5/25.9 MB 31.8 MB/s eta 0:00:01\n","   ---- ----------------------------------- 3.1/25.9 MB 24.9 MB/s eta 0:00:01\n","   ----- ---------------------------------- 3.4/25.9 MB 24.1 MB/s eta 0:00:01\n","   ----- ---------------------------------- 3.4/25.9 MB 24.1 MB/s eta 0:00:01\n","   ----- ---------------------------------- 3.4/25.9 MB 12.9 MB/s eta 0:00:02\n","   ----- ---------------------------------- 3.7/25.9 MB 13.2 MB/s eta 0:00:02\n","   --------- ------------------------------ 5.9/25.9 MB 16.5 MB/s eta 0:00:02\n","   ---------- ----------------------------- 6.7/25.9 MB 16.6 MB/s eta 0:00:02\n","   ----------- ---------------------------- 7.4/25.9 MB 16.2 MB/s eta 0:00:02\n","   ----------- ---------------------------- 7.7/25.9 MB 16.0 MB/s eta 0:00:02\n","   ----------- ---------------------------- 7.7/25.9 MB 16.0 MB/s eta 0:00:02\n","   ----------- ---------------------------- 7.7/25.9 MB 16.0 MB/s eta 0:00:02\n","   ------------ --------------------------- 8.2/25.9 MB 12.8 MB/s eta 0:00:02\n","   ------------- -------------------------- 8.8/25.9 MB 12.9 MB/s eta 0:00:02\n","   -------------- ------------------------- 9.5/25.9 MB 13.0 MB/s eta 0:00:02\n","   --------------- ------------------------ 9.8/25.9 MB 12.5 MB/s eta 0:00:02\n","   --------------- ------------------------ 10.0/25.9 MB 12.0 MB/s eta 0:00:02\n","   ----------------- ---------------------- 11.4/25.9 MB 11.9 MB/s eta 0:00:02\n","   ------------------ --------------------- 12.0/25.9 MB 11.5 MB/s eta 0:00:02\n","   ------------------- -------------------- 12.6/25.9 MB 11.5 MB/s eta 0:00:02\n","   ------------------- -------------------- 12.7/25.9 MB 11.5 MB/s eta 0:00:02\n","   ------------------- -------------------- 12.7/25.9 MB 11.5 MB/s eta 0:00:02\n","   ------------------- -------------------- 12.9/25.9 MB 10.1 MB/s eta 0:00:02\n","   ---------------------- ----------------- 14.5/25.9 MB 12.6 MB/s eta 0:00:01\n","   ----------------------- ---------------- 15.3/25.9 MB 11.9 MB/s eta 0:00:01\n","   ------------------------ --------------- 15.9/25.9 MB 11.3 MB/s eta 0:00:01\n","   ------------------------- -------------- 16.6/25.9 MB 11.1 MB/s eta 0:00:01\n","   -------------------------- ------------- 17.2/25.9 MB 11.1 MB/s eta 0:00:01\n","   --------------------------- ------------ 17.8/25.9 MB 11.1 MB/s eta 0:00:01\n","   ---------------------------- ----------- 18.4/25.9 MB 12.8 MB/s eta 0:00:01\n","   ----------------------------- ---------- 19.0/25.9 MB 12.6 MB/s eta 0:00:01\n","   ------------------------------ --------- 19.6/25.9 MB 12.6 MB/s eta 0:00:01\n","   ------------------------------- -------- 20.2/25.9 MB 13.6 MB/s eta 0:00:01\n","   -------------------------------- ------- 20.8/25.9 MB 13.1 MB/s eta 0:00:01\n","   --------------------------------- ------ 21.4/25.9 MB 12.6 MB/s eta 0:00:01\n","   --------------------------------- ------ 22.0/25.9 MB 12.6 MB/s eta 0:00:01\n","   ---------------------------------- ----- 22.6/25.9 MB 12.6 MB/s eta 0:00:01\n","   ----------------------------------- ---- 23.2/25.9 MB 14.6 MB/s eta 0:00:01\n","   ------------------------------------ --- 23.8/25.9 MB 13.9 MB/s eta 0:00:01\n","   ------------------------------------- -- 24.4/25.9 MB 13.4 MB/s eta 0:00:01\n","   -------------------------------------- - 25.0/25.9 MB 12.8 MB/s eta 0:00:01\n","   ---------------------------------------  25.6/25.9 MB 12.8 MB/s eta 0:00:01\n","   ---------------------------------------- 25.9/25.9 MB 13.1 MB/s eta 0:00:00\n","Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n","   ---------------------------------------- 0.0/240.7 kB ? eta -:--:--\n","   --------------------------------------- 240.7/240.7 kB 15.4 MB/s eta 0:00:00\n","Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n","   ---------------------------------------- 0.0/133.4 kB ? eta -:--:--\n","   ---------------------------------------- 133.4/133.4 kB ? eta 0:00:00\n","Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n","Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n","Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n","   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n","   ---------------------------------------- 60.8/60.8 kB ? eta 0:00:00\n","Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n","   ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n","   ---------------------------------------- 50.7/50.7 kB ? eta 0:00:00\n","Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n","   ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n","Downloading multidict-6.0.5-cp39-cp39-win_amd64.whl (28 kB)\n","Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n","   ---------------------------------------- 0.0/76.9 kB ? eta -:--:--\n","   ---------------------------------------- 76.9/76.9 kB ? eta 0:00:00\n","Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n","Installing collected packages: xxhash, shtab, pyarrow-hotfix, pyarrow, multidict, mdurl, frozenlist, eval-type-backport, docstring-parser, dill, attrs, async-timeout, yarl, multiprocess, markdown-it-py, aiosignal, rich, aiohttp, tyro, datasets, trl\n","Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 datasets-2.20.0 dill-0.3.8 docstring-parser-0.16 eval-type-backport-0.2.0 frozenlist-1.4.1 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 rich-13.7.1 shtab-1.7.1 trl-0.9.4 tyro-0.8.4 xxhash-3.4.1 yarl-1.9.4\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -U trl"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-21T11:32:55.029556Z","iopub.status.busy":"2024-06-21T11:32:55.029148Z","iopub.status.idle":"2024-06-21T11:32:55.755047Z","shell.execute_reply":"2024-06-21T11:32:55.754127Z","shell.execute_reply.started":"2024-06-21T11:32:55.029523Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","\n","hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n","login(token = hf_token)"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-21T11:33:53.847659Z","iopub.status.busy":"2024-06-21T11:33:53.847284Z","iopub.status.idle":"2024-06-21T11:33:53.852007Z","shell.execute_reply":"2024-06-21T11:33:53.851068Z","shell.execute_reply.started":"2024-06-21T11:33:53.847633Z"},"trusted":true},"outputs":[],"source":["base_model = r\"C:\\Users\\satwi\\Downloads\\ChatBot\\Models\\llama-3-transformers-8b-chat-hf-v1\"\n","new_model = r\"C:\\Users\\satwi\\Downloads\\ChatBot\\Models\\llama\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-21T11:33:59.640796Z","iopub.status.busy":"2024-06-21T11:33:59.640432Z","iopub.status.idle":"2024-06-21T11:36:11.846597Z","shell.execute_reply":"2024-06-21T11:36:11.845768Z","shell.execute_reply.started":"2024-06-21T11:33:59.640771Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.95s/it]\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","c:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"]},{"ename":"ZeroDivisionError","evalue":"integer division or modulo by zero","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m base_model_reload, tokenizer \u001b[38;5;241m=\u001b[39m setup_chat_format(base_model_reload, tokenizer)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Merge adapter with base model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_reload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n","File \u001b[1;32mc:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\peft_model.py:271\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[1;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[1;32m--> 271\u001b[0m model\u001b[38;5;241m.\u001b[39mload_adapter(model_id, adapter_name, is_trainable\u001b[38;5;241m=\u001b[39mis_trainable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[1;32mc:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\peft_model.py:581\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[1;34m(self, model_id, adapter_name, is_trainable, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m no_split_module_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_split_modules\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 581\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_split_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_split_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced_low_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    588\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(\n\u001b[0;32m    589\u001b[0m         \u001b[38;5;28mself\u001b[39m, max_memory\u001b[38;5;241m=\u001b[39mmax_memory, no_split_module_classes\u001b[38;5;241m=\u001b[39mno_split_module_classes\n\u001b[0;32m    590\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\satwi\\anaconda3\\envs\\llama\\lib\\site-packages\\accelerate\\utils\\modeling.py:753\u001b[0m, in \u001b[0;36mget_balanced_memory\u001b[1;34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[0m\n\u001b[0;32m    750\u001b[0m     low_zero \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    752\u001b[0m module_sizes \u001b[38;5;241m=\u001b[39m compute_module_sizes(model, dtype\u001b[38;5;241m=\u001b[39mdtype, special_dtypes\u001b[38;5;241m=\u001b[39mspecial_dtypes)\n\u001b[1;32m--> 753\u001b[0m per_gpu \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_devices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_devices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;66;03m# We can't just set the memory to model_size // num_devices as it will end being too small: each GPU will get\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;66;03m# slightly less layers and some layers will end up offload at the end. So this function computes a buffer size to\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# add which is the biggest of:\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;66;03m# - the size of no split block (if applicable)\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;66;03m# - the mean of the layer sizes\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_split_module_classes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"]}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from peft import PeftModel\n","import torch\n","from trl import setup_chat_format\n","# Reload tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","\n","base_model_reload = AutoModelForCausalLM.from_pretrained(\n","        base_model,\n","        low_cpu_mem_usage=True,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\",\n","        trust_remote_code=True,\n",")\n","\n","base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n","\n","# Merge adapter with base model\n","model = PeftModel.from_pretrained(base_model_reload, new_model)\n","\n","model = model.merge_and_unload()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-21T11:37:39.335926Z","iopub.status.busy":"2024-06-21T11:37:39.335574Z","iopub.status.idle":"2024-06-21T11:37:47.499750Z","shell.execute_reply":"2024-06-21T11:37:47.498750Z","shell.execute_reply.started":"2024-06-21T11:37:39.335898Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<|im_start|>user\n","I want to replace my oreder<|im_end|>\n","<|im_start|>assistant\n","I appreciate your desire to replace your order with the number. We apologize for any inconvenience caused. To proceed with the replacement, could you please provide us with the details of the order you would like to replace it with? This will help us ensure a seamless replacement process and provide you with the order you want. Thank you for your patience and cooperation.assistant\n","I appreciate your desire to replace your order with the number. We apologize for any inconvenience caused. To proceed with the replacement, could you please provide us with the details of the order you would like to replace it with? This will\n"]}],"source":["messages = [{\"role\": \"user\", \"content\": \"I want to replace my oreder\"}]\n","\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")\n","\n","outputs = pipe(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n","print(outputs[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-21T11:38:07.969928Z","iopub.status.busy":"2024-06-21T11:38:07.969602Z","iopub.status.idle":"2024-06-21T11:39:04.761544Z","shell.execute_reply":"2024-06-21T11:39:04.760617Z","shell.execute_reply.started":"2024-06-21T11:38:07.969904Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('llama-3-8b-chat-doctor/tokenizer_config.json',\n"," 'llama-3-8b-chat-doctor/special_tokens_map.json',\n"," 'llama-3-8b-chat-doctor/tokenizer.json')"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(\"llama-3-8b-chat-doctor\")\n","tokenizer.save_pretrained(\"llama-3-8b-chat-doctor\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-21T11:39:50.400069Z","iopub.status.busy":"2024-06-21T11:39:50.399053Z","iopub.status.idle":"2024-06-21T11:44:27.916908Z","shell.execute_reply":"2024-06-21T11:44:27.915987Z","shell.execute_reply.started":"2024-06-21T11:39:50.400023Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ce93f788ebd48d1b04cfa3d2a85285e","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03f98888f2f74c1ba651a34c6a2f42ad","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"693405f17dc34cc3916efaa25c3c7c86","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88dbda1ea4044131a828b524603a85c3","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbcb9e93fa3f4e639d8efb9e82b1cd0d","version_major":2,"version_minor":0},"text/plain":["Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53dc67c0367c43898d17adb2d979cfc8","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Alkyema/llama-3-8b-chat-doctor/commit/69d4e1e4d3086a718e0e7e8e2646500b8ac94ed7', commit_message='Upload tokenizer', commit_description='', oid='69d4e1e4d3086a718e0e7e8e2646500b8ac94ed7', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model.push_to_hub(\"llama-3-8b-chat-doctor\", use_temp_dir=False)\n","tokenizer.push_to_hub(\"llama-3-8b-chat-doctor\", use_temp_dir=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-21T11:44:44.584672Z","iopub.status.busy":"2024-06-21T11:44:44.584014Z","iopub.status.idle":"2024-06-21T11:49:09.381009Z","shell.execute_reply":"2024-06-21T11:49:09.379521Z","shell.execute_reply.started":"2024-06-21T11:44:44.584637Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Cloning into 'llama.cpp'...\n","remote: Enumerating objects: 966, done.\u001b[K\n","remote: Counting objects: 100% (966/966), done.\u001b[K\n","remote: Compressing objects: 100% (735/735), done.\u001b[K\n","remote: Total 966 (delta 240), reused 602 (delta 189), pack-reused 0\u001b[K\n","Receiving objects: 100% (966/966), 18.18 MiB | 22.59 MiB/s, done.\n","Resolving deltas: 100% (240/240), done.\n","/kaggle/working/llama.cpp\n"]},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":184722512,"sourceType":"kernelVersion"},{"isSourceIdPinned":true,"modelInstanceId":28083,"sourceId":33551,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}
